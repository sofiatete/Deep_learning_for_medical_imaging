{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1: Deadline: first class in week 4 of the course: 25-2-2025 10:00 (if updated).\n",
    "Hand in this notebook with output. Make sure that it is able to run and produce all the figures and results you show. Also, use the text boxes to answer the questions and interpret your results, relating them to the course materials.\n",
    "\n",
    "\n",
    "Exercises made by Oliver Gurney-Champion. Please contact us via Canvas, or e-mail directly to:\n",
    "Oliver: o.j.gurney-champion@amsterdamumc.nl\n",
    "Matthan: m.w.a.caan@amsterdamumc.nl\n",
    "Dilara: d.tank@amsterdamumc.nl\n",
    "Daan: d.kuppens@amsterdamumc.nl\n",
    "\n",
    "These are a large set of challenging exercises, for which you will get 3 weeks to complete. I would strongly advise you to stick to the suggested schedule, which will ensure you have sufficient knowledge to answer the questions when completing them, and finalize all questions in time.\n",
    "\n",
    "Note that the networks will be lite and can run on your local computer/laptop in short time (minutes). There is no need as yet to run this on Surf, although we highly encourage you to make sure Surf works for you (for exercise sets 2 and 3).\n",
    "\n",
    "# Exercise 1: Program network in PyTorch. (60%)\n",
    "During the class, a brief introduction was given to quantitative imaging. In this exercise, you will program your first neural network that will help estimate quantitative MRI parameters from quantitative data. In particular, we will be looking at the intra-voxel incoherent motion (IVIM) model for diffusion-weighted MRI:\n",
    "\n",
    "S(b)=S_0×( (1-f)×e^-b×D^ +f×e^-b×D*^ )                                                                    [1]\n",
    "\n",
    "With S the measured signal, S0 the baseline signal at S(b=0), f the perfusion fraction, D the diffusion coefficient and D* the pseudo diffusion coefficient. For more information on what the model means exactly and how it is used clinically, I would suggest reading \"Introduction to IVIM MRI | Radiology Key\" (https://radiologykey.com/introduction-to-ivim-mri/). But for the purpose of this exercise, it is just a model.\n",
    "\n",
    "Normally, f, D and D* (named Dp in the code) are obtained by fitting S(b) using least-squares fitting. But these approaches are known to be prone to noise in the data and often produce poor estimates.\n",
    "\n",
    "Therefore, you will write a neural network that predicts f from a given S(b). There are great tools available that take care of training models, such as PyTorch, Karas and Tensorflow. However, for the purpose of this exercise, you will make use of PyTorch. Moreover, we will use weights-and-biasses to keep track of how training is going.\n",
    "\n",
    "# Wednesday 3-4-2024\n",
    "At https://github.com/oliverchampion/AI_for_medical_imaging_course you will find the Python assignment. To help visualize progress and to isolate certain snippets of code, we wrote this as a Jupyter Notebook (exercise1.ipynb). As you can see, we have already provided a data-generator, some plotting tools to plot the training progress. The notebook should run as is and train a neural network! For your first lecture, we suggest you (Wednesday)\n",
    "-\tInstall all prerequisites in your virtual enviroment (requierements.txt)\n",
    "-\tStart a WandB account @ https://wandb.ai/  you will need to log in when running the script\n",
    "-\tGo through the script to see whether you understand what happens.\n",
    "-\tTrain your first neural network .\n",
    "-\tVisualize the results on your WandB page."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4088317021.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 2\u001B[1;36m\u001B[0m\n\u001B[1;33m    Hand in this notebook with output. Make sure that it is able to run and produce all the figures and results you show. Also, use the text boxes to answer the questions and interpret your results, relating them to the course materials.\u001B[0m\n\u001B[1;37m                 ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import requiered packages\n",
    "imports the packages and sets the random seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "seed =42"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "login to your free wandb account. Note you will need to set up your account on https://wandb.ai/authorize\n",
    "wandb allows you to keep track of your neural network training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "%env WANDB_SILENT=True\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulate and view the IVIM data\n",
    "This allows you to study what the data looks like in jupyter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 400, 600]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "## Loading the (simulated) dataset\n",
    "data_sim, D, f, Dp = hf.sim_signal(SNR=(10,30),bvalues=bvalues,sims=30,seed=np.random.randint(1,10000))\n",
    "\n",
    "## plotting some curves and data for visualisation\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i, j].plot(bvalues, data_sim[i+10*j,:], 'o')\n",
    "        datapred=hf.ivim(np.arange(0,np.max(bvalues)), D[i+10*j], f[i+10*j], Dp[i+10*j], 1)\n",
    "        axs[i, j].plot(np.arange(0,np.max(bvalues)), datapred)\n",
    "        axs[i, j].set_ylim(0, 1.2)\n",
    "        axs[i, j].set(xlabel='b-value (s/mm2)', ylabel='normalised signal')\n",
    "plt.legend(('noisy data', 'true curve'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the data into training and validation\n",
    "Here, we split our data into a training set, validation set and test set. Note that the current implementation only uses the training set and it is up to you (in your exercises) to also implement the validation and test run. At this point, we already split the data for you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sim_dat(bvalues,batch_size = 16,SNR=(10,40),sims=1000,seed=np.random.randint(1,10000)):\n",
    "    with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "        data_sim = hf.sim_signal(SNR=SNR,bvalues=bvalues,sims=sims,seed=seed)\n",
    "        # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "        for i in range(4):\n",
    "            #make b-value data pairs\n",
    "            example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "            # put it in a table\n",
    "            table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "            #tell wandb to plot the table\n",
    "            wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "        # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "        #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "        train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "        #split = int(np.floor(len(rest) * 0.5))\n",
    "        #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "        # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "        trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "        inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                       batch_size=min(batch_size,len(val_set)),\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=False)\n",
    "            # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "        testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                       batch_size=min(batch_size,len(test_set)),\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=False)\n",
    "    return trainloader, inferloader, testloader\n",
    "sim_dat(bvalues)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Design a neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# funtion for stacking the layers and making the model\n",
    "def make_model(n_inputs=5,n_hidden=1,n_outputs=1):\n",
    "    #initialize the model object\n",
    "    model = nn.Sequential()\n",
    "    # fill out the model with hidden layers.\n",
    "    for i in range(n_hidden):\n",
    "        # as we loop, we add hidden layers\n",
    "        model.add_module('layer_linear'+str(i), nn.Linear(n_inputs, n_inputs))\n",
    "        # we also add a ReLu layer\n",
    "        model.add_module('layer_ReLu'+str(i), nn.ReLU())\n",
    "    #and a final output layer\n",
    "    model.add_module('last_layer',nn.Linear(n_inputs, n_outputs))\n",
    "    # to ensure positive predictions, we end with a ReLu function before giving output\n",
    "    model.add_module('last',nn.Sigmoid())\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "# function for initializing network weights for individual layers\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train your first network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_network(name, epochs=200, learningrate=0.1, hidden_layers=2, seed=42, optimizer='SGD',bvalues=bvalues,batch_size=16,sims=1000):\n",
    "\n",
    "    trainloader, inferloader, testloader = sim_dat(bvalues,batch_size=batch_size,sims=sims)\n",
    "\n",
    "    model = make_model(n_inputs=len(bvalues), n_hidden=hidden_layers, n_outputs=1)\n",
    "\n",
    "    # initialize model --> we did this above, but during the exercise, you might be re-running this part of the script several times with different settings. This way we make sure you re-initiate the training and don't continue in the last model\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # initialize wandb\n",
    "    wandb.init(\n",
    "            project=\"AI_for_medical_imaging\", job_type=\"training\", name=name)\n",
    "\n",
    "    # set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # probe available devices\n",
    "    if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.determinstic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set default device. If GPU is available, the network will be trained on the GPU. Note that further down in the code, stuff will be sent \".to(device)\" to make sure it is available on the GPU.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # define the loss of the network (mean square error)\n",
    "    loss_module = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "    # the optimizer determines how strongly to update the network's weights based on the calculated loss.\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learningrate)\n",
    "    else:\n",
    "        raise NotImplementedError('this optimizer is not implemented yet...')\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # initiate losses to 0\n",
    "        train_loss_f = 0\n",
    "        val_loss_f = 0\n",
    "        # set model to training such that forward passes are remembered (requiered for backpropogating the loss)\n",
    "        model.train()\n",
    "        #loop over all training data0\n",
    "        SD_train = 0\n",
    "        sys_train = 0\n",
    "        for x in trainloader:\n",
    "            # reset the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            # get data (x[0]) and put the data on the GPU if available\n",
    "            batch=x[0].to(device)\n",
    "            # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "            f_ref = torch.flatten(x[2].to(device))\n",
    "            # put the data through the neural network\n",
    "            f_pred = torch.flatten(model.forward(batch))\n",
    "            # calculate loss (compare predicted f to the ground trueth)\n",
    "            loss_f = loss_module(f_pred, f_ref)\n",
    "            #add found loss to the train loss, to keep track of the loss this epoch\n",
    "            train_loss_f += loss_f.item()\n",
    "            # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "            loss_f.backward()\n",
    "            # update all weights accoording to their derrivatives to the loss.\n",
    "            optimizer.step()\n",
    "            # calculate the standard deviation and systematic error on the trianing data\n",
    "            SD, sys = hf.error_metrics(f_pred.cpu().detach().numpy(),f_ref.cpu().detach().numpy())\n",
    "            # add the errors to ultimately calculate their mean over the trianing data. calculating mean SDs goes via the Root Mean Squares. So add SDs squared\n",
    "            SD_train += SD**2\n",
    "            sys_train += sys\n",
    "        # now divide by the total amount of training data to calculate the mean (sys error) and square of mean (SD).\n",
    "        SD_train = np.sqrt(SD_train/trainloader.__len__())\n",
    "        sys_train = sys_train/trainloader.__len__()\n",
    "        # after training, set model to evaluation mode\n",
    "        model.eval()\n",
    "        # initialize error_metrics\n",
    "        SD_val=0\n",
    "        sys_val=0\n",
    "        ######################your code here for validation loss#########################\n",
    "        #make b-value data pairs: Note these currently contain the f_ref and f_pred from the trianing data. You may want to swap to validation data once implemented\n",
    "        example_data=[[x,y] for (x,y) in zip(f_ref.cpu().detach().numpy(),f_pred.cpu().detach().numpy())]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"f_ref\", \"f_pred\"])\n",
    "        #tell wandb to plot the table\n",
    "        # note that some parameters are being logged which you still need to define in the validation loop!\n",
    "        if epoch % 10 == 0:\n",
    "            wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_train,\"error/systematic error\":sys_train,\"data_plot epoch \" + str(epoch): wandb.plot.scatter(table, \"f_ref\", \"f_pred\", title=f'epoch{epoch}')})\n",
    "\n",
    "        ## print output in terminal. Only useful for debugging when WandB does not work\n",
    "        #print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))\n",
    "    wandb.finish()\n",
    "    #return val_test SD_test\n",
    "\n",
    "train_network('test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First week (on schedule means finish by Friday) [if updated: 7-2-2025]\n",
    "For these exercises, you will want to produce new cells that generate the outputs. The notebook cells should run and produce the figures without input of the examiners.\n",
    "\n",
    "## 1A-C: train the neural network\n",
    "Adapt the script above for your exercises A-C such that it produces the desired results and plots. Describe the results/intepertation in this text baloon.\n",
    "\n",
    "A.\tThe current network implementation only looks at training data. This means that the network’s performance is over-estimated. Please use the validation set to monitor performance during training (note that we have already put the model on evaluation mode in line 59). At what point is the network fully trained? Explain how you know this. Show the effect from overfitting and underfitting.\n",
    "B.\tSimilarly, use the test dataset to test for final performance. Explain why this is needed.\n",
    "C.\tCurrently, standard gradient descent optimizer is being used to train the network, with a learning rate of 0.01. Investigate the performance of the network for different  optimizer (i.e. adam loss was discussed in the lecture) and explain what you see. What does the Adam  optimizer do differently from the SGD  optimizer that would make it perform better/differently?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "D.\tReturn back to SGD. Test how the performance depends on the learning rate. This can be done by plotting the systematic (sys_test) and random (SD_test) errors as function of learning rate (10<LR<0.0000001; steps in order of magnitude; e.g. 10, 1, 0.1, ….).\n",
    "##### Plot the performance (sys_val and SD_val) as function of the learning rate and add this to your report. Explain what you see (hint: take a look at the loss curves).\n",
    "easiest is to save the final performance and plot it in the jupyter notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "E.\tPlot the performance (sys_val and SD_val) as function of the width (number of neurons per layer: 5, 10, 20, 50, 100) and depth (number of hidden layers: 1, 2, ..., 8) of the network and add these to the report. Discuss how width and depth may influence the network; if it has this behaviour in your data, highlight it; if not, explain why it may not occur in your dataset.\n",
    "\n",
    "Hint: currently, the width of the network copies the width of the data, so you need to uncouple the input width from the network width by adapting the \"make model\" code.\n",
    "Tip: you only need to plot different widths for 1 depth (e.g. 2) and different depths for 1 width (e.g. 10).\n",
    "Tip: possibly some effects get clearer when more training data is simulated (\"sims\" in data_sim). But also note that having too much training data may hide some of the effects from other exercises, so don't forget to revert it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "F.\tTry different batch sizes (1, 4, 16, 64, 128, 516).Explain the behavior of the network you see.What is the effect of having smaller batches? And larger batches?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "G.Chose 3 hidden layers, width of 40,  learning rate of 0.1 and batch size of 2. Now train the network for 3000 epochs. At what point in the network fully trained? How do you see this? Does any overfitting occur?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Second week (on schedule means finish by Wednesday) [if updated: 12-2-2025]\n",
    "H.\tCurrently, the model uses a Relu activation function. Test the effect of different activation funtions on the network performance. Show how well does a sigmoid or ELU work (i.e. what is the effect on performance)?\n",
    "\n",
    "Note, you can either:\n",
    "- adapt the scripts above to program this \"neatly\" as input parameter.\n",
    "- redefine new \"programs\" below that have the new properties you want.\n",
    "\n",
    "Note that in the case of option 1, your programs need to stay backwards compatible, as examiners will need to be able to rerun your code and reproduce your results!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far, we simualted a small amount of data, as it is easier to show overfitting etc. Note that you may want to simulate somewhat more data for this and the following exercises. This can be achieved by setting sims=100000 for the following exercises\n",
    "\n",
    "I.\tLet the network also predict D and Dp (note x[1] and x[3] are D and Dp respectively). Show the loss curve of D and Dp and explain how you can see that they have been implemented properly/the model is learning them.\n",
    "o\tThe network will need more than 1 output --> tip, use loss_D.backward(retain_graph=True) for the first two losses to remember losses and propogate all 3 losses backward in turn\n",
    "o\tAlternatively, you could train 3 networks simultaniously\n",
    "Note that 0<f<1 on avergae is orders of magnitude larger than D and Dp. To ensure all three losses equally affect the network weights you may want to enlarge the loss of D and Dp by multiplying them with some value.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "J.\tUse the sigmoid in the final layer to constrain 0.5e-3 <D< 3.5e-3; 0<f<1; 5e-3<D*<130e-3. Explain how you did this.\n",
    "Note: D, f and D* are in very different parameter value ranges, and hence their RMS is too. A network will focus on the largest loss. You may want to scale the RMS to similar ranges for the network to consider all 3 parameters during optmizing.\n",
    "Also note: You may want to play with hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# For scoring 7.5+\n",
    "So far, you have been working with simulated data (taken care of by Data_loader.py). For such data, we know the ground truth values. However, in vivo, we have no way of knowing the ground truth. How will our network perform? Note that for this exercise you may need ot play with hyperparameters and design choices to train.\n",
    "\n",
    "K.\tUse the network, as trained in (J) and apply it to real-world data which is provided by running “dataval, valid_id, bvalues = hf.load_real_data(eval=True)”.\n",
    "- You will need to export you trained network in the return of the \"train_network\" function\n",
    "- you will need to apply it to the \"dataval\" from hf.load_real_data(eval=True)\n",
    "- you will then want to put the outputs through \"hf.plot_example(np.squeeze(D_out), valid_id,0.003)\", \"hf.plot_example(np.squeeze(f_out), valid_id,0.7)\" and \"hf.plot_example(np.squeeze(Dp_out), valid_id,0.1)\", with the predicted D, f and Dp being D_out, f_out and Dp_out.\n",
    "\n",
    "Note that alongside your plot (the first), also a conventional least squares fit is provided as a reference. Show the resulting parameter maps. How does your approach compare? Why do you think your particular approach would look better/worse?\n",
    "\n",
    "L.\tIdeally, you would train your network on real-world data. However, in this particular case, it is hard to get gold standard references. Luckily, we can use our understanding of physics, and of how stuff “should behave” to work our way around this. You will redesign your network loss, such that it can train on data without any gold standard references! Instead of placing the L2 loss on f_pred v.s. f_ref. Currently, the network is learning to minimize the difference between predicted fpred and the ground truth referene ftrue. In vivo, we may not have these references. To overcome this, we will now introduce a physics-informed loss. Use the IVIM equation [1] to propogate the predictions (D, f and Dp) into the signal space (S). Then, take e.g. the mean-square-error between the predicted signal and the input signal. Note that you will need to use torch functions (instead of numpy functions) to ensure you can backpropogate the loss through the equation into the network. You can train this network on the simulated data from earlier exercises. But it should also be able to train it on the in vivo data from \"datatrain, bvalues = hf.load_real_data(eval=False)\". This ensures that the network is use to looking at \"real\" data. Optimize the network’s training using the real data provided (“test_in_vivo.py”; datatrain). Evaluate the network on the same data as in 1 (data, valid_id, bvalues = dl.load_real_data(eval=True)). How does it perform?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datatrain, bvalues = hf.load_real_data(eval=False)\n",
    "dataval, valid_id, bvalues = hf.load_real_data(eval=True)\n",
    "print('training data is ' + str(len(datatrain)) + ' long')\n",
    "\n",
    "''''your code here'''\n",
    "### use datatrain to train your network.\n",
    "## then test your network using dataval.\n",
    "''''if you manage to make predictions of D, f and Dp, the following code will allow you to plot them:'''\n",
    "\n",
    "hf.plot_example(np.squeeze(D_out), valid_id,0.003)\n",
    "hf.plot_example(np.squeeze(f_out), valid_id,0.7)\n",
    "hf.plot_example(np.squeeze(Dp_out), valid_id,0.1)\n",
    "hf.plot_ref()\n",
    "\n",
    "### you can compare supervised and self-supervised fits"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
